{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** The softmax algorithm ** \n",
    "- We need to make our bandit algorithm care about the known differences betweeen the estimated values of the arms when our algorithm decides decides which arm to explore.\n",
    "- We need _**structured exploration**_ rather than the _**haphazard exploration**_ that the epsilon-Greedy algorithm provides.\n",
    "- The softmax algorithm tries to cope up with arms differing in estimated value by explicitly incorporating information about the reward rates of the available arms into its method of choosing which arm to select when it explores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "import nbimporter\n",
    "import epsilonGreedy\n",
    "import DebuggingBanditAlgorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def categorical_draw(probs):\n",
    "    z = random.random()\n",
    "    cum_prob = 0.0\n",
    "    for idx, prob in enumerate( probs ):\n",
    "        cum_prob += prob\n",
    "        if cum_prob > z :\n",
    "            return idx        \n",
    "    return idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SoftMax():\n",
    "    \n",
    "    '''\n",
    "        This class implements Softmax Multiarm Bandit algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__( self, temperature, counts=[], values=[] ):\n",
    "        '''\n",
    "            temperature : Systems behave randomly at high temperature, while they take on more structure at low temperature\n",
    "           \n",
    "            counts  : A vector of integers of length N that tells us how many time we have played \n",
    "            each of the N arms available to us in the current bandit problem.\n",
    "            \n",
    "            values  : A vector of floating point numbers that defines the average amount of reward we have gotten \n",
    "            when playing each of the N arms available to us.\n",
    "        '''\n",
    "        self.temperature = temperature\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'EpsilonGreedy({:.2f},{!r}, {!r})'.format(self.temperature, self.counts, self.values)\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        # Intializing / reset rewards & counts to zeros for each arm(or option)\n",
    "        self.counts = [ 0 for col in range(n_arms) ]\n",
    "        self.values = [ 0.0 for col in range(n_arms) ]\n",
    "    \n",
    "    def select_arm(self):\n",
    "        ''' Returns the index of the next arm to pull '''\n",
    "        values = [ numpy.exp( v / self.temperature ) for v in self.values ]\n",
    "        z = sum( values )\n",
    "        probs = [ v / z for v in values ]\n",
    "        return categorical_draw(probs)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        '''        \n",
    "        After we pull an arm, we get a reward signal back from our system. This function update our algorithm's beliefs\n",
    "        about the quality of the arm we just chose by providing this reward information.\n",
    "        \n",
    "        chosen_arm : The numeric index of the most recently chosen arm\n",
    "        reward     : The reward received from chossing that arm\n",
    "        '''\n",
    "        self.counts[ chosen_arm ] += 1\n",
    "        n = self.counts[ chosen_arm ]\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ( ( n-1 ) * value + reward ) / float(n) \n",
    "        self.values[chosen_arm] = new_value\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from epsilonGreedy import EpsilonGreedy\n",
    "from DebuggingBanditAlgorithms import BernoulliArm\n",
    "from DebuggingBanditAlgorithms import testing_algorithm\n",
    "random.seed(1)\n",
    "means = [ 0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = [  BernoulliArm(mu) for mu in means ]\n",
    "\n",
    "f = open( 'standard_softmax_results.tsv', 'w')\n",
    "for temperature in [ 0.1, 0.2, 0.3, 0.4, 0.5] :\n",
    "    algo = SoftMax ( temperature )\n",
    "    algo.initialize(n_arms)\n",
    "    results = testing_algorithm ( algo, arms, 5000, 250 )\n",
    "    for i in range( len(results[0]) ):\n",
    "        f.write( str(temperature)  + '\\t')\n",
    "        f.write( '\\t'.join( [ str( results[j][i]) for j in range( len(results) ) ] )  + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Annealing Softmax Algorithm\n",
    "- Annealing is the process of decreasing the temperature in a Softmax algorithm over the time.\n",
    "- Annealing is the process of modifying a bandit algorithm's behavior so that it will explore less over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AnnealingSoftMax():\n",
    "    \n",
    "    '''\n",
    "        This class implements Softmax Multiarm Bandit algorithm\n",
    "    '''\n",
    "\n",
    "    def __init__( self,  counts=[], values=[] ):\n",
    "        '''\n",
    "            temperature : Systems behave randomly at high temperature, while they take on more structure at low temperature\n",
    "           \n",
    "            counts  : A vector of integers of length N that tells us how many time we have played \n",
    "            each of the N arms available to us in the current bandit problem.\n",
    "            \n",
    "            values  : A vector of floating point numbers that defines the average amount of reward we have gotten \n",
    "            when playing each of the N arms available to us.\n",
    "        '''\n",
    "        # self.temperature = temperature\n",
    "        self.counts = counts\n",
    "        self.values = values\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return 'EpsilonGreedy({!r}, {!r})'.format( self.counts, self.values)\n",
    "    \n",
    "    def initialize(self, n_arms):\n",
    "        # Intializing / reset rewards & counts to zeros for each arm(or option)\n",
    "        self.counts = [ 0 for col in range(n_arms) ]\n",
    "        self.values = [ 0.0 for col in range(n_arms) ]\n",
    "    \n",
    "    def select_arm(self):\n",
    "        ''' Returns the index of the next arm to pull '''\n",
    "        t = sum(self.counts) + 1\n",
    "        temperature = 1 / math.log(t + 0.0000001 )\n",
    "        values = [ numpy.exp( v / temperature ) for v in self.values ]\n",
    "        z = sum( values )\n",
    "        probs = [ v / z for v in values ]\n",
    "        return categorical_draw(probs)\n",
    "    \n",
    "    def update(self, chosen_arm, reward):\n",
    "        '''        \n",
    "        After we pull an arm, we get a reward signal back from our system. This function update our algorithm's beliefs\n",
    "        about the quality of the arm we just chose by providing this reward information.\n",
    "        \n",
    "        chosen_arm : The numeric index of the most recently chosen arm\n",
    "        reward     : The reward received from chossing that arm\n",
    "        '''\n",
    "        self.counts[ chosen_arm ] += 1\n",
    "        n = self.counts[ chosen_arm ]\n",
    "        value = self.values[chosen_arm]\n",
    "        new_value = ( ( n-1 ) * value + reward ) / float(n) \n",
    "        self.values[chosen_arm] = new_value\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import random\n",
    "import math\n",
    "\n",
    "import nbimporter\n",
    "import epsilonGreedy\n",
    "import DebuggingBanditAlgorithms\n",
    "\n",
    "from epsilonGreedy import EpsilonGreedy\n",
    "from DebuggingBanditAlgorithms import BernoulliArm\n",
    "from DebuggingBanditAlgorithms import testing_algorithm\n",
    "random.seed(1)\n",
    "means = [ 0.1, 0.1, 0.1, 0.1, 0.9]\n",
    "n_arms = len(means)\n",
    "random.shuffle(means)\n",
    "arms = [  BernoulliArm(mu) for mu in means ]\n",
    "\n",
    "f = open( 'annealing_softmax_results.tsv', 'w')\n",
    "\n",
    "algo = AnnealingSoftMax ( )\n",
    "algo.initialize(n_arms)\n",
    "results = testing_algorithm ( algo, arms, 5000, 250 )\n",
    "for i in range( len(results[0]) ):\n",
    "    f.write( '\\t'.join( [ str( results[j][i]) for j in range( len(results) ) ] )  + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
