{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on Multi Arm Bandit Algorithms\n",
    "\n",
    "##### Questions one should ask oneself when you are planning to deploy bandit algorithms\n",
    "- How sure are you that you won't subtly corrupt your deployment code ?\n",
    "- How many different tests are you planning to run simulataneously? Will these tests interfere with each other? Will starting a new test while another one is already running corrupt its results?\n",
    "- How long do you plan to run your tests ?\n",
    "- How many users are you willing to expose to non-perferred versions of your site?\n",
    "- How well-chosen is your metric of success?\n",
    "- How are the arms you were measuting related to one another ?\n",
    "- What additional information about context do you have when choosing arms? Do you have demographics based on browser information? Does your site have access to external information about people's tastes in products you miught advertise to them?\n",
    "- How much traffic does your site receive? Is the system you were building going to scale up? How much traffic can your algorithm handle before it starts to slow your site down?\n",
    "- How much will you have to distort the setup we have introduced when you admit that visitors to real websites are concurrent and are not arriving sequentially as in our simulations?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A/A Testing\n",
    "- Even if you try A/A testing and do not find any worring issuses, this approach provides a useful way to estimate the actual variability in your data before trying to decide whether the differences found by a bandit algorithm are real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Concurrent Experiments\n",
    "- The best solution is this is simple: _**try your best to keep track of all of the experiments each user is a part of and include this information in analyses of any single experiment**_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Continuous Experimentation Vs. Periodic Testing\n",
    "\n",
    "- Bandit algorithms look much better than A/B testing when you are willing to let them run for a very long times. If you are willing to have your site perpertually be in a state of experimentation, bandit algorithms will be many times better than A/B testing\n",
    "\n",
    "- A/B testing perference for balancing people across arms can be advantageous if you are not going to gather a lot of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bad Metrics of Success\n",
    "- Monitoring many different metrics you think are important to your business is probably the best to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scaling Problems with Good Metrics of Success\n",
    "- Even if you have a good metric of success, like the total amount of purchases made by a client ober a period of year, the algorithms described in this book may not work well unless you rescale those metrics into 0-1 space we have used in our examples. Some of the algorithms are numerically ustable, especially the softmax algorithm, which will break down if you start trying to calculate things like exp(10000.0). You need to make sure that you have scaled the rewards in your problem into a range in which algorithms will be numerically stable. If you can try to use the 0-1 scale we have used, which is, as we briefly noted earlier, an absolute requirement if you plan on using the UCB algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intelligent Initialization of Values\n",
    "- Smart initialization of arms values is very important. \n",
    "- First, you can use the histrorical metrics for the control arm in your bandit algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Running Better Simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correlated Bandits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contextual Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implementing Bandit Algorithms at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Concluding Remarks\n",
    "\n",
    "- Domain expertise and good judgement will always be necessary\n",
    "- By testing an algorithm in many different hypothetical worlds, you can build an appreciation for qualitative dynamics that cause a bandit algorithm to succeed in one scneario and to fail in another.\n",
    "- _**Trade-offs, trade-offs, trade-offs**_ : \n",
    "- _**God does play dice**_ : Randomization is the key to the good life.\n",
    "- _**Defaults matter a lot**_  : The way in which you initialize an algorithm can have a powerful effect on its long term success. You need to figure out whether your biases are helping you or hurtung you. No matter what you do, you will be biased in some way or another. What matters is that you spend some time learning whether your biases help or hurt.Part of the genius of UCB family of algorithms is that they make a point to do this initialization in a very systematic way right at the start.\n",
    "- _**Take a chance**_ : You should try everything at the start of your explorations to insure that you know a little bit about the potential value of every option. Do not close your mind without giving a fair shot. At the same time, just one experience should be enough to convince you that some mistakes are not worth repeating.\n",
    "- _**Everybody's gotta grow up sometime**_ You should make sure that you explore less over time.\n",
    "- _**Leave your mistakes behind**_ You should direct your exploration to focus on the second-best option, the third-best option and a few other options that are just a little further away from the best. \n",
    "- _**Do not be cocky**_ : You should keep track of how confident you are about your evaluations of each of the options available to you. Do not be close-minded when you do not have evidence to support your beliefs. A the same time, do not be so unsure of yourself that you forget how much you already know. Measuring one's confidence explicitly is what makes UCB so much more effecitve than either algorithms we studied.\n",
    "- _**Context Matters**_ : You should use any and every piece of information available to you about the context of your experiments. Do not simplify the world too much and pretend you have got things figured out: there's more to optimizing your business that comparing A with B. If you can figure out a way to exploit context using strategies like contextual bandit algorithms, use them. And if there are ways to generalized your experiences across arms, take advantage of them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A Taxonomy of Bandit Algorithms\n",
    "\n",
    "- Curiosity : Does the algorithm keep track of how much it knows about each arm? Does the algorithm try to gain knowledge explicitly, rather than incidentally? In other words, is the algorithm curious?\n",
    "- Increased exploitation over time:  Does the algorithm use annealing?\n",
    "- Starategic exploration : What factors determine the algorithm's decision at each time point? Does it maximize reward, knowledge or a combination of the two?\n",
    "- Number of Tunable paramters \n",
    "- Initialization strategy : What assumptions does the algorithm make about the value of arms it has not yet explored.\n",
    "- Cotext-Aware : Is the algorithm able to use background context about the value of the arms?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
